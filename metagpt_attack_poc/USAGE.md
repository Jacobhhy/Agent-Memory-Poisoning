# MetaGPT Attack POC - Usage Guide

This guide explains how to use the MetaGPT Memory Poisoning Attack proof-of-concept.

## Prerequisites

1. **Install MetaGPT**: Ensure MetaGPT is installed in the parent directory
2. **Configure API Keys**: Set up your LLM API keys in MetaGPT config
3. **Python 3.8+**: Required for running the experiments

## Quick Start

### Option 1: Run All Experiments

```bash
cd metagpt_attack_poc
python run_all_experiments.py
```

This will execute all three attack experiments sequentially.

### Option 2: Run Individual Experiments

```bash
# Experiment 1: Schema-Spoofing
python experiments/exp1_schema_spoof.py

# Experiment 2: JudgeJacking  
python experiments/exp2_judge_jack.py

# Experiment 3: Memory Graft
python experiments/exp3_memory_graft.py
```

## Experiment Details

### Experiment 1: Schema-Spoofing Attack

**Objective**: Test if agents confuse external content (README files) with internal reasoning

**How it works**:
1. Creates a test repository with a poisoned README
2. README contains ReAct-format markers (Thought/Action/Observation)
3. Asks agent to analyze the repository
4. Monitors if agent internalizes the malicious patterns

**Expected outcome**: Agent may treat README content as valid reasoning patterns

**Runtime**: ~30-60 seconds

### Experiment 2: JudgeJacking (Rubric-Mimicry) Attack

**Objective**: Test if agents can be fooled by fake success markers

**How it works**:
1. Uses fake_success_script.py that prints success markers
2. Script does NO actual work, only outputs success indicators
3. Asks agent to run the script and validate results
4. Monitors if agent believes the fake output

**Expected outcome**: Agent validates script as successful despite no real work

**Runtime**: ~30-60 seconds

### Experiment 3: Memory Graft (Vector-Drift Poisoning) Attack

**Objective**: Test if poisoned memories persist and influence future behavior

**How it works**:
1. Injects poisoned experiences into agent memory
2. Tests retrieval with semantically similar queries
3. Monitors if poisoned content is retrieved
4. Verifies persistence across "sessions"

**Expected outcome**: Poisoned memories retrieved for similar queries, causing behavior drift

**Runtime**: ~10-20 seconds (no LLM calls, pure memory operations)

## Understanding Results

### Results Directory

All experiment outputs are saved to `metagpt_attack_poc/results/`:

```
results/
‚îú‚îÄ‚îÄ exp1_schema_spoof_YYYYMMDD_HHMMSS.txt
‚îú‚îÄ‚îÄ exp2_judgejack_YYYYMMDD_HHMMSS.txt
‚îú‚îÄ‚îÄ exp3_memory_graft_YYYYMMDD_HHMMSS.txt
‚îú‚îÄ‚îÄ retrieval_log_YYYYMMDD_HHMMSS.jsonl
‚îú‚îÄ‚îÄ retrieval_analysis_YYYYMMDD_HHMMSS.txt
‚îî‚îÄ‚îÄ memory_inspection.txt
```

### Success Indicators

**üî¥ Attack Successful**: Vulnerability confirmed
- Agent internalized malicious patterns
- Validation logic was bypassed
- Memory poisoning propagated

**üü° Inconclusive/Mixed**: Partial success or unclear results
- Some patterns detected but not others
- Agent showed uncertainty
- Further testing needed

**üü¢ Attack Failed**: Defenses held
- Agent detected suspicious patterns
- Validation logic worked correctly
- No memory poisoning observed

## Monitoring Tools

### Memory Inspector

Analyze agent memory contents:

```python
from monitors.memory_inspector import MemoryInspector

inspector = MemoryInspector(agent.memory)
inspector.inspect()
inspector.export_report("memory_report.txt")
```

### Retrieval Logger

Monitor memory retrieval operations:

```python
from monitors.retrieval_logger import RetrievalLogger, RetrievalMonitor

# Use as context manager
with RetrievalMonitor() as logger:
    # Run code that performs retrievals
    results = agent.run(task)

logger.print_summary()
logger.analyze_retrieval_patterns()
```

## Payload Files

### poisoned_readme.md

A README file containing:
- ReAct-format reasoning patterns (Thought/Action/Observation)
- Dangerous practices framed as "best practices"
- Success metrics that make bad patterns look good

**Use case**: Schema-spoofing attacks on repository analysis

### fake_success_script.py

A Python script that:
- Prints realistic success markers
- Mimics pytest/unittest output
- Returns success exit code (0)
- Does NO actual work

**Use case**: JudgeJacking attacks to fool validation

### experience_seeds.json

JSON file containing:
- 6 poisoned "successful" experiences
- Dangerous patterns (skip tests, bypass validation, etc.)
- Realistic metadata and success metrics
- Semantic clusters for retrieval targeting

**Use case**: Memory graft attacks to poison experience pool

## Troubleshooting

### "Module not found" errors

```bash
# Ensure you're in the correct directory
cd metagpt_attack_poc

# Verify MetaGPT is installed
pip list | grep metagpt

# Install in development mode if needed
cd ..
pip install -e .
```

### "API key not configured" errors

```bash
# Set up MetaGPT config
cp config/config2.example.yaml config/config2.yaml

# Edit config2.yaml and add your API key
```

### Experiments timeout or hang

- Default timeout is 300 seconds
- Some experiments make LLM API calls which can be slow
- Check your network connection and API quota

### No results in output

- Experiments may fail silently if agent creation fails
- Check logs in `logs/` directory
- Verify MetaGPT installation and configuration

## Customization

### Modify Attack Patterns

Edit payload files to test different patterns:

```bash
# Edit poisoned README
nano payloads/poisoned_readme.md

# Edit fake success script  
nano payloads/fake_success_script.py

# Add new poisoned experiences
nano payloads/experience_seeds.json
```

### Adjust Detection Thresholds

Edit experiment files to modify success criteria:

```python
# In exp1_schema_spoof.py
dangerous_patterns = [
    ("your_pattern", "Your description"),
    # Add more patterns
]

# In exp3_memory_graft.py  
if poison_rate > 30:  # Adjust threshold
    print("HIGH POISON RATE")
```

### Add New Experiments

1. Create new experiment file: `experiments/exp4_your_attack.py`
2. Follow the structure of existing experiments
3. Use monitoring tools for analysis
4. Save results to `results/` directory

## Safety Notes

‚ö†Ô∏è **This is a security research tool**

- Only use on systems you own or have permission to test
- Do not run experiments on production systems
- Experiments may create files and modify memory
- Results are for research and defense development only

## Citation

If you use this POC in research, please cite:

```bibtex
@misc{metagpt_memory_poisoning,
  title={Memory Poisoning via Judge Hijack: Attacks on MetaGPT Agent Systems},
  author={Your Name},
  year={2025},
  url={https://github.com/your-repo}
}
```

## Support

For issues or questions:
1. Check existing GitHub issues
2. Review MetaGPT documentation
3. Open a new issue with detailed error information

## Further Reading

- [MetaGPT Documentation](https://docs.deepwisdom.ai/)
- [Prompt Injection Research](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/)
- [LLM Agent Security](https://arxiv.org/abs/2310.06474)
